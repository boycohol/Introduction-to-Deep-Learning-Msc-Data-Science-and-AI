{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 36px;\">Vanishing Gradients / Overfitting / Save and Load</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Student information:\n",
    "- Name: Tuan Anh NGUYEN\n",
    "- Email: tuan.nguyen@etu.univ-cotedazur.fr\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tasks:\n",
    "\n",
    "- Using Kereas tuner Bayesan approach,\n",
    "    + find the best hyper-paramters\n",
    "    + for the « fashion_mnist » dataset\n",
    "    + Metrics = accuracy\n",
    "    + And upload your notebook and your model on Moodle\n",
    "\n",
    "1. Read fashion_mnist dataset\n",
    "2. Build the model and search best hyper-parameters with keras tuner\n",
    "3. Fit the best model and evaluate it\n",
    "4. Save the best model\n",
    "5. Try to load the best model\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Read fashion_mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "X_train = X_train.reshape(len(X_train), -1)/255 # Reshape dataset and normalize it\n",
    "X_test = X_test.reshape(len(X_test), -1)/255 # Reshape dataset and normalize it\n",
    "# Some basic verification about labels\n",
    "nb_classes = len(np.unique(y_train))\n",
    "assert np.min(y_train) == 0 and np.max(y_train) == nb_classes-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Build the model and search best hyper-parameters with keras tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):          # Wrap a model in a function\n",
    "    # Define hyper parameters\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "    hp_activation = hp.Choice('activation', ['relu', 'sigmoid'])\n",
    "    inputs = Input(shape=(X_train.shape[1], ))              # Input layer\n",
    "    x = Dense(hp_units, activation=hp_activation)(inputs)   # Hidden layer\n",
    "    outputs = Dense(10, activation='softmax')(x)\n",
    "    \n",
    "    # Define the model\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the tuner and search the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_search_params():\n",
    "    tuner = kt.BayesianOptimization(model_builder,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,   # total number of model configurations to test\n",
    "    seed=None,\n",
    "    tune_new_entries=True,\n",
    "    allow_new_entries=True, max_retries_per_trial=0,\n",
    "    directory='Labs',\n",
    "    project_name='test_tuner')\n",
    "    # Search for hyper-parameters\n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "    tuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[es])\n",
    "\n",
    "    return tuner, es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from Labs\\test_tuner\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner, es = build_search_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "units:256\n",
      "activation:sigmoid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "# All hyperpameters are on best_hps.values\n",
    "for k, v in best_hps.values.items():\n",
    "    print(f\"{k}:{v}\")\n",
    "\n",
    "# Buid the  model with the best hyperparemeters \n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "# Get the best model \n",
    "best_model= tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Fit the best model and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_evaluate():\n",
    "    # fit the best_model (as usual)\n",
    "    hist = best_model.fit(X_train, y_train, epochs=100, validation_split=0.2,\n",
    "    callbacks=[es])\n",
    "    # Babysit the best model (as usual)\n",
    "    pd.DataFrame({'loss':hist.history['loss'], 'val_loss':hist.history['val_loss']}).plot()\n",
    "    \n",
    "    # predict with the best model (as usual)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    # Evaluate the model (as usual)\n",
    "    eval = best_model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: TuanAnhNGUYEN/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: TuanAnhNGUYEN/model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'TuanAnhNGUYEN/model'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 784), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1738301474064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1738361799600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1738301647760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1738361148288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "best_model.export(\"TuanAnhNGUYEN/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Try to load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_new_model():\n",
    "    layers = keras.layers.TFSMLayer('TuanAnhNGUYEN/model')\n",
    "    inputs = keras.layers.Input(shape=(X_train.shape[1],))\n",
    "    outputs = layers(inputs)\n",
    "    model = keras.models.Model(inputs, outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = load_new_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honestly, I'm still struggling with loading the new model and using it. Could you send me some feedback of my code:\n",
    "- Is there any problem with my model?\n",
    "- Did I export the model in the wrong way? \n",
    "- Could you give the way using the saved model? \n",
    "\n",
    "Thanks for your feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##### Conclusion:\n",
    "- Through this part, we've learned how to use KerasTuner, avoiding the overfitting or vanishing gradient problem. Also, we know how to save and load the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- Tensorflow keras documentations: https://www.tensorflow.org/api_docs/python/tf/keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
